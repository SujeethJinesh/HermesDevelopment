# MVP-0 F0.3 T0.3: Deterministic Evaluation Harness Implementation Summary

## What Changed

### Code Added
- `eval/run_arms.py` (385 lines): Main evaluation harness with CLI, config parity enforcement, and metrics emission
- `eval/_seed.py` (147 lines): Deterministic seeding utilities for all RNGs
- `eval/__init__.py` (3 lines): Package initialization
- `tests/test_run_arms_parity.py` (216 lines): Unit tests for config parity and determinism
- `tests/integration/test_run_arms_determinism.py` (374 lines): Integration tests for end-to-end determinism

### Key Features Implemented
1. **Strict Config Parity**: Only `configs/generation.yaml` allowed, no CLI overrides permitted
2. **Deterministic Execution**: All RNGs seeded, deterministic task seeds computed from base seed
3. **Hermetic Integration**: Uses `env.hermetic.HermeticRun` for isolated execution with network blocking
4. **Metrics Emission**: Outputs both `metrics.jsonl` (incremental) and `summary.parquet` (final)
5. **Warmup Exclusion**: First 5 inferences excluded from tokens/s measurements

## Why

The evaluation harness is critical for reproducible benchmarking of Arms in the HERMES system. Without strict config parity and deterministic execution, results would not be comparable across runs or environments. The hermetic execution ensures tests don't access external resources, preventing non-deterministic network effects.

## How It Works

### Architecture
```
eval.run_arms CLI
    ├── Config Parity Check (reject overrides)
    ├── Seed All RNGs (Python, NumPy, PyTorch if present)
    ├── For each task:
    │   ├── Compute task-specific seed (hash of base_seed + task_id)
    │   ├── Create HermeticRun environment
    │   ├── Execute task in isolated worktree + venv
    │   ├── Collect metrics (excluding warmup)
    │   └── Write incremental metrics to JSONL
    └── Write final summary to Parquet
```

### Determinism Strategy
1. **Global Seeding**: `seed_all()` seeds Python random, NumPy, PyTorch, and PYTHONHASHSEED
2. **Task Seeds**: Each task gets `compute_task_seed(base_seed, task_id)` for reproducible per-task randomness
3. **Run IDs**: Generated deterministically from seed via `generate_deterministic_id()`
4. **Config Hash**: SHA256 of config file content for verification

### Config Parity Enforcement
```python
if gen_cfg_path != "configs/generation.yaml":
    raise ConfigParityError("Only configs/generation.yaml allowed")

# Also check for common override attempts
for attr in ["temperature", "top_p", "max_tokens", ...]:
    if hasattr(args, attr):
        raise ConfigParityError(f"Override attempted: --{attr}")
```

## Tests Run

### Unit Tests (9 tests, all passing)
```bash
$ python3 -m pytest tests/test_run_arms_parity.py -q
.........                                                                [100%]
```

### Integration Tests (determinism verification)
```bash
$ HERMES_HERMETIC=1 python3 -m pytest tests/integration/test_run_arms_determinism.py::TestRunArmsDeterminism::test_identical_runs_produce_identical_metrics -v
tests/integration/test_run_arms_determinism.py .                         [100%]
============================== 1 passed in 14.42s ==============================
```

## Metrics Impact

### Determinism Verification
Two identical runs (same seed=123) produce:
- **Identical metrics** (excluding timestamps): ✓ Verified
- **Identical Parquet hash**: `2f1cc621cd315b82` (both runs)
- **Identical run IDs**: `arm_A_123_60b57f75500b` (deterministic from seed)

### Performance Metrics (toy tasks)
- **E2E latency p50**: 2652 ms
- **E2E latency p95**: 2704 ms
- **Sandbox setup p50**: ~3000 ms
- **Sandbox cleanup p95**: ~120 ms
- **Config hash**: `19d2bcc56010777f` (stable across runs)

### Schema Compliance
All required fields present in Parquet output:
- `task_id`, `arm`, `seed`, `hermetic`
- `bytes_out`, `bytes_in`, `tokens_out`, `tokens_in`
- `prefill_tokens`, `decode_tokens`, `e2e_latency_ms`
- `message_path_ms`, `pass`, `run_id`, `config_hash`

## Deviations from Spec

None. All acceptance criteria met:
1. ✓ Determinism: Identical runs produce byte-identical output (timestamps excluded)
2. ✓ Config Parity: Only `configs/generation.yaml` accepted, overrides rejected
3. ✓ Hermetic: Integrated with `env.hermetic` for network-blocked execution
4. ✓ Metrics: Both JSONL and Parquet emitted with correct schema

## Next Steps

1. **Implement real Arms** (A, C, PM, D1, D1_SAE) - currently using mock metrics
2. **Integrate SWE-bench tasks** - load actual tasks instead of toy tasks
3. **Add warmup calibration** - verify 5 inference warmup is sufficient
4. **Production metrics** - connect to actual LLM clients for real token measurements
5. **Benchmark execution** - run full SWE-bench Lite suite with all Arms

## Evidence Pack

### 1. Config Parity Enforcement
```bash
$ python3 -m eval.run_arms --arm A --seed 123 --gen_cfg configs/custom.yaml --toy 1
ERROR: Config parity violation: only 'configs/generation.yaml' is allowed, got 'configs/custom.yaml'. No overrides permitted.
```

### 2. Determinism Proof
```python
# Two runs with same seed produce identical metrics
✓ DataFrames equal (excluding timestamps): True
✓ Hash run1: 2f1cc621cd315b82
✓ Hash run2: 2f1cc621cd315b82
✓ Hashes match: True
✓ DETERMINISM VERIFIED: Two identical runs produced identical output!
```

### 3. Hermetic Execution
```python
# From HermeticRun manifest
"hermetic": true,
"scratch_path": "scratch/toy-000/arm_A_123_60b57f75500b_toy-000",
"worktree_path": "scratch/toy-000/arm_A_123_60b57f75500b_toy-000/worktree",
"venv_path": "scratch/toy-000/arm_A_123_60b57f75500b_toy-000/venv",
"network_guard_install_ms": 0.834375
```

### 4. Files Changed
- Added: `eval/run_arms.py`, `eval/_seed.py`, `eval/__init__.py`
- Added: `tests/test_run_arms_parity.py`, `tests/integration/test_run_arms_determinism.py`
- Created: `docs/MVP0/F0.3/T0.3_summary.md`

## Acceptance: PASS ✓

All T0.3 acceptance criteria met. The evaluation harness provides deterministic, hermetic execution with strict config parity and proper metrics emission.