# MVP-1 F1.1 T1.2 Summary - Arm PM with MCP Anchors

**Task:** T1.2 - Implement Arm PM (Protobuf + MCP Anchors)  
**Milestone:** MVP-1  
**Feature:** F1.1 - MCP Anchors  
**Commit:** 5e69c47  
**Branch:** sujinesh/M1_F1_T12  
**Status:** FAIL - Acceptance Criteria Not Met

## Acceptance Criteria Status

| Criterion | Target | Current | Status | Evidence |
|-----------|--------|---------|--------|----------|
| **Bytes/solve** | PM < C | NOT MEASURED | ❌ FAIL | Hermetic evaluation not executed |
| **Pass@1** | Within ±2pp of C | NOT MEASURED | ❌ FAIL | Hermetic evaluation not executed |
| **MCP deref p95** | < 50ms | 42ms (simulated) | ⚠️ PARTIAL | Unit test only, not production measurement |
| **Hermetic runs** | Deterministic | NOT EXECUTED | ❌ FAIL | Infrastructure ready but runs not performed |
| **No synthetic content** | None in source | REMOVED | ✅ PASS | CI guard enforces ban, verified by make check-toy |

## What Changed

### Added Files
- `agents/real_tester.py` - Real test runner that executes pytest with subprocess
- `scripts/prepare_swebench_data.py` - HF dataset cache preparation for offline runs
- `.github/workflows/ban_toy_datasets.yml` - CI guard against toy/synthetic patterns

### Modified Files  
- `Makefile` - Uses --instances_file instead of --smoke, includes self-check
- `eval/datasets/swebench_lite.py` - Added hermetic mode with HF_DATASETS_OFFLINE support
- `agents/pm_arm.py` - Cleaned to remove all synthetic generation
- `tests/pm/test_pm_bytes_vs_c_on_slice20.py` - Updated to simulate real output

### Removed Files
- `agents/pm_arm_enhanced.py` - Deleted (synthetic content)
- `agents/pm_shared_content.py` - Deleted (synthetic content)  
- `scripts/demo_mcp_benefit.py` - Deleted (synthetic demo)

## Evidence Pack

### Diff Summary
```
git diff --stat 00bc197..5e69c47
 .github/workflows/ban_toy_datasets.yml     | 59 +++++++++
 Makefile                                    | 12 +-
 agents/pm_arm.py                           | 400 ++---------
 agents/real_tester.py                      | 341 ++++++++++
 docs/MVP1/F1.1/T1.2_summary_final.md       | 188 ++++++
 eval/datasets/swebench_lite.py             | 35 +-
 scripts/prepare_swebench_data.py           | 125 ++++
 tests/pm/test_pm_bytes_vs_c_on_slice20.py  | 37 +-
 8 files changed, 819 insertions(+), 378 deletions(-)
```

### Unit Test Output
```
pytest tests/pm/test_pm_bytes_vs_c_on_slice20.py -v
============================= test session starts ==============================
platform darwin -- Python 3.11.6, pytest-7.4.3
collected 3 items

tests/pm/test_pm_bytes_vs_c_on_slice20.py::TestPMBytesVsC::test_pm_anchors_large_logs PASSED
tests/pm/test_pm_bytes_vs_c_on_slice20.py::TestPMBytesVsC::test_c_inlines_test_logs PASSED
tests/pm/test_pm_bytes_vs_c_on_slice20.py::TestPMBytesVsC::test_pm_config_threshold PASSED

============================== 3 passed in 0.12s ===============================
```

### Integration Test Logs
NOT EXECUTED - Hermetic runs on slice20 pending

### Metrics JSON
```json
{
  "bytes_per_solve": null,
  "tokens_prefill": null,
  "tokens_decode": null,
  "e2e_latency_ms_p50": null,
  "e2e_latency_ms_p95": null,
  "message_path_ms_p95": null,
  "mcp_deref_ms_p95": 42,
  "sae_accept_rate": null,
  "rollback_ms_p95": null,
  "pass_at_1": null
}
```

### Run Manifest
```json
{
  "model_sha": null,
  "tokenizer_sha": null,
  "quantization": null,
  "base_repo_sha": "5e69c47",
  "run_repo_sha": "5e69c47",
  "config_hash": null,
  "lockfile_sha": null,
  "os_fingerprint": "Darwin-24.6.0-arm64",
  "python_version": "3.11.6",
  "seed": null,
  "venv_hash": null,
  "hermetic": false
}
```

### Hermetic Confirmation
NOT EXECUTED - Runs pending

### SWE-bench Lite Fidelity

The loader (`eval/datasets/swebench_lite.py`) uses the official dataset:
- Dataset: `SWE-bench/SWE-bench_Lite` (canonical)
- Assertions: `len(dev)==23`, `len(test)==300` enforced
- Fields: All required columns validated (`instance_id`, `repo`, `base_commit`, `test_patch`, etc.)
- Hermetic: `HF_DATASETS_OFFLINE=1` support implemented

## CI Guard Proof

```bash
$ make check-toy
Checking for banned toy/smoke patterns in source code...
✅ No banned patterns found in source code
```

## Run Commands

```bash
# Prepare HF cache (one-time, online)
python scripts/prepare_swebench_data.py \
  --dataset SWE-bench/SWE-bench_Lite \
  --cache_dir ~/.cache/hf/swebench_lite \
  --verify

# Hermetic evaluation (to be executed)
export HF_DATASETS_OFFLINE=1
export HF_HOME=~/.cache/hf/swebench_lite
export HF_DATASETS_CACHE=~/.cache/hf/swebench_lite/datasets
export HERMES_HERMETIC=1

python -m eval.run_arms \
  --arm C \
  --dataset swebench_lite \
  --instances_file configs/swebench_lite_slice20.txt \
  --gen_cfg configs/generation.yaml \
  --seed 123 \
  --hermetic on

python -m eval.run_arms \
  --arm PM \
  --dataset swebench_lite \
  --instances_file configs/swebench_lite_slice20.txt \
  --gen_cfg configs/generation.yaml \
  --seed 123 \
  --hermetic on
```

## Deviations from Spec

1. **Hermetic evaluation not executed**: Infrastructure is ready but actual runs on slice20 are pending
2. **Bytes/solve and pass@1 not measured**: Requires hermetic evaluation completion
3. **MCP deref p95 from unit test only**: Production measurement requires full evaluation

## Next Steps to Meet Acceptance

1. **Execute hermetic evaluation**:
   - Run prepare_swebench_data.py to cache dataset
   - Execute C and PM arms on slice20 with identical seeds
   - Verify deterministic outputs

2. **Integrate real_tester with PM**:
   - Wire real_tester.py to generate actual pytest output
   - Ensure output exceeds 32KB to trigger anchoring
   - Store via MCP with proper TTLs

3. **Measure and report metrics**:
   - Calculate bytes_per_solve for both arms
   - Verify PM < C on average
   - Ensure pass@1 within ±2pp

## Conclusion

The implementation has successfully:
- ✅ Removed all synthetic content from source code
- ✅ Implemented real test runner with actual pytest execution
- ✅ Created hermetic infrastructure with HF offline support
- ✅ Established CI guards against toy/synthetic patterns

However, acceptance criteria are NOT MET because hermetic evaluation has not been executed. The infrastructure is ready but requires actual runs to measure bytes/solve and pass@1 metrics.